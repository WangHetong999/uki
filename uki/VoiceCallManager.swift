//
//  VoiceCallManager.swift
//  uki
//
//  è¯­éŸ³é€šè¯ç®¡ç†å™¨ - è´Ÿè´£è¯­éŸ³è¯†åˆ«ã€éŸ³é¢‘å¤„ç†ã€å¯¹è¯æµç¨‹
//

import Foundation
import Speech
import AVFoundation
import Combine

// MARK: - é€šè¯çŠ¶æ€

enum CallState: Equatable {
    case idle              // ç©ºé—²
    case connecting        // è¿æ¥ä¸­
    case connected         // å·²è¿æ¥
    case listening         // æ­£åœ¨è†å¬ç”¨æˆ·
    case userSpeaking      // ç”¨æˆ·æ­£åœ¨è¯´è¯
    case aiSpeaking        // AI æ­£åœ¨è¯´è¯
    case error(String)     // é”™è¯¯çŠ¶æ€
    case ended             // å·²ç»“æŸ

    // æ‰‹åŠ¨å®ç° Equatable åè®®
    static func == (lhs: CallState, rhs: CallState) -> Bool {
        switch (lhs, rhs) {
        case (.idle, .idle),
             (.connecting, .connecting),
             (.connected, .connected),
             (.listening, .listening),
             (.userSpeaking, .userSpeaking),
             (.aiSpeaking, .aiSpeaking),
             (.ended, .ended):
            return true
        case (.error, .error):
            return true  // åªæ¯”è¾ƒç±»å‹ï¼Œä¸æ¯”è¾ƒé”™è¯¯ä¿¡æ¯
        default:
            return false
        }
    }
}

// MARK: - è¯­éŸ³é€šè¯ç®¡ç†å™¨

class VoiceCallManager: NSObject, ObservableObject {

    // MARK: - Published çŠ¶æ€

    @Published var callState: CallState = .idle
    @Published var recognizedText: String = ""
    @Published var audioLevel: Float = 0.0
    @Published var isMuted: Bool = false
    @Published var isSpeakerOn: Bool = false
    @Published var errorMessage: String?

    // MARK: - è¯­éŸ³è¯†åˆ«

    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?

    // MARK: - éŸ³é¢‘å¼•æ“

    private let audioEngine = AVAudioEngine()
    private var audioPlayer: AVAudioPlayer?

    // MARK: - ç½‘ç»œæœåŠ¡

    private let networkService = NetworkService()

    // MARK: - å¯¹è¯æ§åˆ¶

    private var silenceTimer: Timer?
    private var lastSpeechTime: Date?
    private let silenceThreshold: TimeInterval = 1.5  // 1.5ç§’é™é»˜åå‘é€
    private var isProcessingAI = false

    // MARK: - åˆå§‹åŒ–

    override init() {
        super.init()
        print("ğŸ¤ VoiceCallManager åˆå§‹åŒ–")
    }

    deinit {
        print("ğŸ¤ VoiceCallManager é‡Šæ”¾")
        endCall()
    }

    // MARK: - æƒé™è¯·æ±‚

    /// è¯·æ±‚éº¦å…‹é£å’Œè¯­éŸ³è¯†åˆ«æƒé™
    func requestPermissions() async -> Bool {
        // 1. è¯·æ±‚éº¦å…‹é£æƒé™
        let micStatus = await requestMicrophonePermission()
        guard micStatus else {
            await MainActor.run {
                errorMessage = "éœ€è¦éº¦å…‹é£æƒé™æ‰èƒ½è¿›è¡Œè¯­éŸ³é€šè¯"
                callState = .error("éº¦å…‹é£æƒé™è¢«æ‹’ç»")
            }
            return false
        }

        // 2. è¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™
        let speechStatus = await requestSpeechRecognitionPermission()
        guard speechStatus else {
            await MainActor.run {
                errorMessage = "éœ€è¦è¯­éŸ³è¯†åˆ«æƒé™æ‰èƒ½è¿›è¡Œè¯­éŸ³é€šè¯"
                callState = .error("è¯­éŸ³è¯†åˆ«æƒé™è¢«æ‹’ç»")
            }
            return false
        }

        print("âœ… æƒé™è¯·æ±‚æˆåŠŸ")
        return true
    }

    private func requestMicrophonePermission() async -> Bool {
        let granted = await withCheckedContinuation { continuation in
            AVAudioSession.sharedInstance().requestRecordPermission { granted in
                continuation.resume(returning: granted)
            }
        }
        print("ğŸ” éº¦å…‹é£æƒé™: \(granted ? "âœ… å·²æˆäºˆ" : "âŒ è¢«æ‹’ç»")")
        return granted
    }

    private func requestSpeechRecognitionPermission() async -> Bool {
        let status = await withCheckedContinuation { (continuation: CheckedContinuation<SFSpeechRecognizerAuthorizationStatus, Never>) in
            SFSpeechRecognizer.requestAuthorization { status in
                continuation.resume(returning: status)
            }
        }
        let granted = (status == .authorized)
        print("ğŸ” è¯­éŸ³è¯†åˆ«æƒé™: \(status.rawValue) - \(granted ? "âœ… å·²æˆäºˆ" : "âŒ è¢«æ‹’ç»")")
        return granted
    }

    // MARK: - é€šè¯æ§åˆ¶

    /// å¼€å§‹é€šè¯
    func startCall() async {
        print("ğŸ“ å¼€å§‹é€šè¯")

        await MainActor.run {
            callState = .connecting
        }

        // è¯·æ±‚æƒé™
        let hasPermissions = await requestPermissions()
        guard hasPermissions else {
            return
        }

        // é…ç½®éŸ³é¢‘ä¼šè¯
        do {
            try configureAudioSession()
        } catch {
            await MainActor.run {
                errorMessage = "éŸ³é¢‘é…ç½®å¤±è´¥: \(error.localizedDescription)"
                callState = .error("éŸ³é¢‘é…ç½®å¤±è´¥")
            }
            return
        }

        // å¯åŠ¨è¯­éŸ³è¯†åˆ«
        do {
            try startSpeechRecognition()
            await MainActor.run {
                callState = .connected
                callState = .listening
            }
            print("âœ… é€šè¯å·²å»ºç«‹ï¼Œå¼€å§‹ç›‘å¬")
        } catch {
            await MainActor.run {
                errorMessage = "è¯­éŸ³è¯†åˆ«å¯åŠ¨å¤±è´¥: \(error.localizedDescription)"
                callState = .error("è¯­éŸ³è¯†åˆ«å¤±è´¥")
            }
        }
    }

    /// ç»“æŸé€šè¯
    func endCall() {
        print("ğŸ“ å¼€å§‹ç»“æŸé€šè¯...")

        // åœ¨åå°çº¿ç¨‹æ‰§è¡Œæ¸…ç†ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self = self else { return }

            self.stopSpeechRecognition()
            self.stopAudioEngine()

            DispatchQueue.main.async {
                self.silenceTimer?.invalidate()
                self.silenceTimer = nil
                self.callState = .ended
                print("âœ… é€šè¯å·²ç»“æŸ")
            }
        }
    }

    // MARK: - éŸ³é¢‘ä¼šè¯é…ç½®

    private func configureAudioSession() throws {
        let audioSession = AVAudioSession.sharedInstance()
        try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.defaultToSpeaker, .allowBluetooth])
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)

        print("âœ… éŸ³é¢‘ä¼šè¯é…ç½®å®Œæˆ")
    }

    // MARK: - è¯­éŸ³è¯†åˆ«

    private func startSpeechRecognition() throws {
        // å¦‚æœä¹‹å‰æœ‰ä»»åŠ¡åœ¨è¿è¡Œï¼Œå…ˆåœæ­¢
        stopSpeechRecognition()

        // åˆ›å»ºè¯†åˆ«è¯·æ±‚
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let recognitionRequest = recognitionRequest else {
            throw NSError(domain: "VoiceCallManager", code: -1, userInfo: [NSLocalizedDescriptionKey: "æ— æ³•åˆ›å»ºè¯†åˆ«è¯·æ±‚"])
        }

        recognitionRequest.shouldReportPartialResults = true
        recognitionRequest.requiresOnDeviceRecognition = false

        // è·å–éŸ³é¢‘è¾“å…¥èŠ‚ç‚¹
        let inputNode = audioEngine.inputNode

        // å¯åŠ¨è¯†åˆ«ä»»åŠ¡
        recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            guard let self = self else { return }

            if let result = result {
                let transcription = result.bestTranscription.formattedString

                DispatchQueue.main.async {
                    self.recognizedText = transcription
                    self.lastSpeechTime = Date()

                    // ç”¨æˆ·æ­£åœ¨è¯´è¯
                    if self.callState == .listening {
                        self.callState = .userSpeaking
                    }
                }

                // é‡ç½®é™é»˜è®¡æ—¶å™¨
                self.resetSilenceTimer()

                print("ğŸ¤ è¯†åˆ«æ–‡å­—: \(transcription)")
            }

            if let error = error {
                print("âŒ è¯†åˆ«é”™è¯¯: \(error.localizedDescription)")
            }
        }

        // é…ç½®éŸ³é¢‘æ ¼å¼
        let recordingFormat = inputNode.outputFormat(forBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, _ in
            self?.recognitionRequest?.append(buffer)

            // è®¡ç®—éŸ³é¢‘ç”µå¹³ï¼ˆç”¨äºæ³¢å½¢æ˜¾ç¤ºï¼‰
            self?.calculateAudioLevel(from: buffer)
        }

        // å¯åŠ¨éŸ³é¢‘å¼•æ“
        print("ğŸ§ å‡†å¤‡å¯åŠ¨éŸ³é¢‘å¼•æ“...")
        audioEngine.prepare()
        try audioEngine.start()
        print("ğŸ§ éŸ³é¢‘å¼•æ“å·²å¯åŠ¨ï¼Œè¿è¡Œä¸­: \(audioEngine.isRunning)")

        print("âœ… è¯­éŸ³è¯†åˆ«å·²å¯åŠ¨ï¼Œè¯†åˆ«å™¨å¯ç”¨: \(speechRecognizer != nil)")
    }

    private func stopSpeechRecognition() {
        recognitionTask?.cancel()
        recognitionTask = nil
        recognitionRequest?.endAudio()
        recognitionRequest = nil

        print("â¹ï¸ è¯­éŸ³è¯†åˆ«å·²åœæ­¢")
    }

    private func stopAudioEngine() {
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
            print("â¹ï¸ éŸ³é¢‘å¼•æ“å·²åœæ­¢")
        }
    }

    // MARK: - é™é»˜æ£€æµ‹

    private func resetSilenceTimer() {
        silenceTimer?.invalidate()

        silenceTimer = Timer.scheduledTimer(withTimeInterval: silenceThreshold, repeats: false) { [weak self] _ in
            self?.onSilenceDetected()
        }
    }

    private func onSilenceDetected() {
        guard !recognizedText.isEmpty, !isProcessingAI else { return }

        print("ğŸ”• æ£€æµ‹åˆ°é™é»˜ï¼Œå‘é€æ–‡å­—åˆ° AI: \(recognizedText)")

        let textToSend = recognizedText
        recognizedText = ""  // æ¸…ç©º

        // å‘é€åˆ° AI
        Task {
            await sendToAI(textToSend)
        }
    }

    // MARK: - AI å¯¹è¯

    private func sendToAI(_ text: String) async {
        guard !isProcessingAI else {
            print("âš ï¸ AI æ­£åœ¨å¤„ç†ä¸­ï¼Œè·³è¿‡")
            return
        }

        isProcessingAI = true

        await MainActor.run {
            callState = .aiSpeaking
        }

        // æš‚åœè¯­éŸ³è¯†åˆ«
        stopSpeechRecognition()

        do {
            // è°ƒç”¨ AI æ¥å£ï¼ˆæµå¼ï¼‰
            let aiReply = try await networkService.sendMessageStream(text, history: []) { chunk in
                print("ğŸ’¬ AI å›å¤: \(chunk)", terminator: "")
            }

            print("\nâœ… AI å®Œæ•´å›å¤: \(aiReply)")

            // è½¬æ¢ä¸ºè¯­éŸ³å¹¶æ’­æ”¾
            try await playAIVoice(aiReply)

            // æ’­æ”¾å®Œæˆï¼Œæ¢å¤ç›‘å¬
            await MainActor.run {
                callState = .listening
            }

            // é‡æ–°å¯åŠ¨è¯­éŸ³è¯†åˆ«
            try startSpeechRecognition()

        } catch {
            print("âŒ AI å¯¹è¯å¤±è´¥: \(error.localizedDescription)")

            await MainActor.run {
                errorMessage = "AI å›å¤å¤±è´¥"
                callState = .listening
            }

            // æ¢å¤è¯­éŸ³è¯†åˆ«
            try? startSpeechRecognition()
        }

        isProcessingAI = false
    }

    private func playAIVoice(_ text: String) async throws {
        print("ğŸ”Š å¼€å§‹æ’­æ”¾ AI è¯­éŸ³")

        // è°ƒç”¨ TTS æ¥å£
        try await networkService.textToSpeech(text)

        print("âœ… AI è¯­éŸ³æ’­æ”¾å®Œæˆ")
    }

    // MARK: - éŸ³é¢‘ç”µå¹³è®¡ç®—

    private func calculateAudioLevel(from buffer: AVAudioPCMBuffer) {
        guard let channelData = buffer.floatChannelData else { return }

        let channelDataValue = channelData.pointee
        let channelDataValueArray = stride(from: 0, to: Int(buffer.frameLength), by: buffer.stride).map { channelDataValue[$0] }

        let rms = sqrt(channelDataValueArray.map { $0 * $0 }.reduce(0, +) / Float(buffer.frameLength))
        let avgPower = 20 * log10(rms)
        let normalizedLevel = max(0, min(1, (avgPower + 50) / 50))

        DispatchQueue.main.async {
            self.audioLevel = normalizedLevel
        }
    }

    // MARK: - é™éŸ³/å…ææ§åˆ¶

    func toggleMute() {
        isMuted.toggle()

        if isMuted {
            // é™éŸ³ï¼šåœæ­¢è¯­éŸ³è¯†åˆ«
            stopSpeechRecognition()
            print("ğŸ”‡ å·²é™éŸ³")
        } else {
            // å–æ¶ˆé™éŸ³ï¼šæ¢å¤è¯­éŸ³è¯†åˆ«
            try? startSpeechRecognition()
            print("ğŸ”Š å·²å–æ¶ˆé™éŸ³")
        }
    }

    func toggleSpeaker() {
        isSpeakerOn.toggle()

        do {
            let audioSession = AVAudioSession.sharedInstance()
            if isSpeakerOn {
                try audioSession.overrideOutputAudioPort(.speaker)
                print("ğŸ“¢ å·²åˆ‡æ¢åˆ°æ‰¬å£°å™¨")
            } else {
                try audioSession.overrideOutputAudioPort(.none)
                print("ğŸ§ å·²åˆ‡æ¢åˆ°å¬ç­’")
            }
        } catch {
            print("âŒ åˆ‡æ¢éŸ³é¢‘è¾“å‡ºå¤±è´¥: \(error.localizedDescription)")
        }
    }
}
